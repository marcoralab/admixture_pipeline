from snakemake.utils import min_version
min_version("7.0")
configfile: "config/config.yaml"

import re

# # check if running on minerva
# import os
# if os.path.exists('/sc/arion'):
#     minerva = True
# else:
#     minerva = False
# 
# if minerva:
#    shell.prefix('ml proxies; ')

# Check for internet

from urllib.request import urlopen
from urllib.error import URLError

try:
    response = urlopen('https://www.google.com/', timeout=10)
    iconnect = True
except URLError as ex:
    iconnect = False

iconnect = iconnect and not ('nointernet' in config and config['nointernet'])

class dummyprovider:
    def remote(string_, allow_redirects="foo", immediate_close="bar"):
        nohttp = string_.removeprefix("http://").removeprefix("https://")
        noftp = nohttp.removeprefix("ftp://").removeprefix("ftps://")
        return noftp

if snakemake.__version__.startswith("7."):
    from snakemake.remote.FTP import RemoteProvider as FTPRemoteProvider
    from snakemake.remote.HTTP import RemoteProvider as HTTPRemoteProvider
    H_ = HTTPRemoteProvider()
    F_ = FTPRemoteProvider()

    def strip_protocol(cls, string_, *args, **kwargs):
        if string_.startswith("ftp://") or string_.startswith("ftps://"):
            url = string_.removeprefix("ftp://").removeprefix("ftps://")
        elif string_.startswith("http://") or string_.startswith("https://"):
            url = string_.removeprefix("http://").removeprefix("https://")
        else:
            url = string_
        return cls.remote(url, *args, **kwargs)

    if iconnect:
        class FTP:
            @staticmethod
            def remote(string_, *args, **kwargs):
                return strip_protocol(F_, string_, *args, **kwargs)
        class HTTP:
            @staticmethod
            def remote(string_, *args, **kwargs):
                return strip_protocol(H_, string_, *args, **kwargs)
    else:
        FTP = dummyprovider
        HTTP = dummyprovider
    class storage:
        @staticmethod
        def ftp(string_, *args, **kwargs):
            return FTP.remote(string_, *args, **kwargs)
        @staticmethod
        def http(string_, *args, **kwargs):
            return HTTP.remote(string_, *args, **kwargs)
else:
    if iconnect:
        class FTP:
            @staticmethod
            def remote(string_, allow_redirects="foo", immediate_close="bar"):
                if not string_startswith("ftp://") and not string_startswith("ftps://"):
                    string_ = "ftp://" + string_
                return storage.ftp(string_)
        class HTTP:
            @staticmethod
            def remote(string_, allow_redirects="foo", immediate_close="bar"):
                if not string_startswith("http://") and not string_startswith("https://"):
                    string_ = "https://" + string_
                return storage.http(string_)
    else:
        FTP = dummyprovider
        HTTP = dummyprovider
        class storage:
            @staticmethod
            def ftp(string_, *args, **kwargs):
                return FTP.remote(string_, *args, **kwargs)
            @staticmethod
            def http(string_, *args, **kwargs):
                return HTTP.remote(string_, *args, **kwargs)

conf = config['datasets']

def get_build(conf, dataset):
    if conf[dataset].get('impute', False):
        return 'hg38'
    else:
        return conf[dataset]['build']

def get_intype(conf, dataset):
    fname = conf[dataset]['infile']
    if re.match(r'.+\.vcf(\.b{0,1}gz)*$', fname):
        return 'vcf'
    elif re.match(r'.+\.bcf$', fname):
        return 'bcf'
    elif re.match(r'.+\.bed$', fname):
        return 'bplink'
    elif re.match(r'.+\.ped$', fname):
        return 'plink'
    else:
        raise Exception('Unrecognized file type')

def get_in(conf, dataset):
    fname = conf[dataset]['infile']
    fname = fname.format(indir=config['indir'], chrom='{chrom}') #allow
    intype = get_intype(conf, dataset)
    if intype in ['plink', 'bplink']:
        return fname[0:-4]
    else:
        return fname

ref_fasta = {"hg19": "resources/ref/human_g1k_hg19.fasta",
             "hg38": "resources/ref/human_gatk_GRCh38.fasta"}

tgbase = "http://ftp-trace.ncbi.nih.gov/1000genomes/ftp/"
tgbase_38 = ("https://storage.googleapis.com/" +
             "gcp-public-data--broad-references/hg38/v0/")

tgfasta = dict(
    hg19=(tgbase + "technical/reference/human_g1k_v37.fasta.gz"),
    GRCh38=(tgbase_38 + 'Homo_sapiens_assembly38.fasta'))

def fasta_remote(wc):
    gbuild = wc.gbuild
    return HTTP.remote(tgfasta[gbuild])

def fasta_md5(wc):
    gbuild = wc.gbuild
    if gbuild == 'GRCh38':
        return '7ff134953dcca8c8997453bbb80b6b5e'
    elif gbuild == 'hg19':
        return '45f81df94f0408d082363e34a081ed81'

if 'pipeline_versions' not in config:
    config['pipeline_versions'] = {}

if 'prefix' in config['pipeline_versions']:
    local_src = os.path.normpath(config['pipeline_versions']['prefix'])
else:
    local_src = 'workflow/modules'

DATASETS = [x for x in conf.keys()]
BUILD = [get_build(conf, x) for x in DATASETS] # hg19 or hg38
filter_dict = {x: conf[x].get('filters', '') for x in DATASETS}
input_type_dict = {x: get_intype(conf, x) for x in DATASETS}
input_dict = {x: get_in(conf, x) for x in DATASETS}
impute_datasets = [x for x in DATASETS if conf[x].get('impute', False)]
impute_build = [conf[x]['build'] for x in DATASETS if conf[x].get('impute', False)]
ADMIXTURE_REF = 'gnomad-hgdp-1kg'
if len(impute_build) > 0:
    assert len(set(impute_build)) == 1, "There can only be one build for imputation"
    impute_build = impute_build[0]
    assert impute_build in ['hg19', 'hg38']
else:
    impute_build = 'hg19'
impute_fasta = ref_fasta[impute_build]

def dict_settings(setting):
    return {k: v for k, v in zip(DATASETS, setting)}

build_dict = dict_settings(BUILD)

wildcard_constraints:
    build='hg(19|38)',
    #cohort='|'.join(DATASETS)

rule all:
    input:
        expand('results/admixture_cohort-{cohort}_ref-{admixstem}.{build}.html',
               zip, admixstem=[ADMIXTURE_REF for x in DATASETS], build=BUILD, cohort=DATASETS),

rule download_tg_fa:
    input: fasta_remote
    output:
        "resources/ref/human_{source}_{gbuild}.fasta",
        "resources/ref/human_{source}_{gbuild}.fasta.fai"
    params:
        md5 = lambda wc: fasta_md5(wc)
    resources:
        mem_mb = 10000,
        time_min = 30
    conda: 'envs/bcftools.yaml'
    #cache: True
    localrule: True
    shell:
        '''
md5sum -c <(echo {params.md5} {input[0]})
if [[ "{input[0]}" == *.gz ]]; then
  zcat {input[0]} > {output[0]} && rstatus=0 || rstatus=$?; true
  if [ $rstatus -ne 2 && $rstatus -ne 0 ]; then
    exit $rstatus
  fi
else
  cp {input[0]} {output[0]}
fi
samtools faidx {output[0]}
'''

def get_intype_(wc):
    intype_ =  wc['inputtype']
    if intype_ == 'bplink':
        return 'bfile'
    elif intype_ == 'plink':
        return 'file'
    else:
        raise Exception('Invalid input type')

def get_infile(wc):
    intype_ = wc['inputtype']
    stem = input_dict[wc['cohort']]
    if intype_ == 'bplink':
        return multiext(stem, ".bed", ".bim", ".fam")
    elif intype_ == 'plink':
        return multiext(stem, ".ped", ".map")
    else:
        raise Exception('Invalid input type')

rule plink_filter:
    input: get_infile
    output: temp(multiext("intermediate/start/{cohort}.{build}.from{inputtype}", ".bed", ".bim", ".fam"))
    params:
        ins = lambda wc: input_dict[wc['cohort']],
        out = "intermediate/start/{cohort}.{build}.from{inputtype}",
        filt = lambda wc: filter_dict[wc['cohort']],
        ftype = get_intype_
    resources:
        mem_mb = 10000,
        time_min = 30
    conda: "envs/plink.yaml"
    shell:
        """
plink --keep-allele-order --{params.ftype} {params.ins} {params.filt} \
  --make-bed --out {params.out}
"""

def preimpute_files(wc):
    ds = wc['cohort']
    intype = input_type_dict[ds]
    stem = expand("intermediate/start/{cohort}.{build}.from{inputtype}",
        inputtype=intype, build=impute_build, cohort=ds)[0]
    return {x: stem + '.' + x for x in ['bed', 'bim', 'fam']}

rule link_impute:
    input: unpack(preimpute_files)
    output:
        bed = "intermediate/impute_start/{cohort}.bed",
        bim = "intermediate/impute_start/{cohort}.bim",
        fam = "intermediate/impute_start/{cohort}.fam"
    resources:
        mem_mb = 1024,
        time_min = 30
    shell:
        '''
cp {input.bed} {output.bed}
cp {input.bim} {output.bim}
cp {input.fam} {output.fam}
'''
    

config_impute = config["imputation"]
config_impute["chroms"] = "1:22"
config_impute["directory"] = "intermediate/impute_start/"
config_impute["SAMPLES"] = impute_datasets
config_impute["out_dir"] = "output/imputation"
config_impute['ref'] = impute_fasta
config_impute['imputation']['default']['build'] = impute_build
config_impute['imputation']['default']['refpanel'] = 'topmed-r3'
if not "outputs" in config_impute:
    config_impute["outputs"] = ['stat_report', 'vcf_bycohort']

if 'imputePipeline' not in config['pipeline_versions']:
    config['pipeline_versions']['imputePipeline'] = 'b40c3521634334c801ed3381769ee8146b049f96'
elif config['pipeline_versions']['imputePipeline'] == 'local':
    config['pipeline_versions']['imputePipeline'] = os.path.join(local_src, 'imputePipeline')
version_impute = config['pipeline_versions']['imputePipeline']

if re.search(r'/', version_impute):
    snakefile_impute = os.path.join(
        os.path.normpath(version_impute), 'workflow', 'Snakefile')
else:
    snakefile_impute = github("marcoralab/imputePipeline",
        path="workflow/Snakefile", tag=version_impute)

if 'postImpute' in config['pipeline_versions']:
    if config['pipeline_versions']['postImpute'] == 'local':
        config['pipeline_versions']['postImpute'] = os.path.join(local_src, 'postImpute')
    else:
        config_impute['version_postImpute'] = config['pipeline_versions']['postImpute']

if impute_datasets:
    module imputation:
        snakefile: snakefile_impute
        config: config_impute

    use rule * from imputation as imputation_*

rule Sample_Flip:
    input:
        bim = "intermediate/start/{cohort}.{build}.from{inputtype}" + '.bim',
        bed = "intermediate/start/{cohort}.{build}.from{inputtype}" + '.bed',
        fam = "intermediate/start/{cohort}.{build}.from{inputtype}" + '.fam',
        fasta = lambda wc: ref_fasta[wc["build"]]
    output:
        temp(multiext("intermediate/cohort_proc/{cohort}.{build}.from{inputtype}_flipped", ".bim", ".bed", ".fam"))
    params:
        stem = "intermediate/cohort_proc/{cohort}.{build}.from{inputtype}"
    resources:
        mem_mb = 10000,
        time_min = 30
    container: 'docker://befh/flippyr:0.5.3'
    shell: "flippyr -p {input.fasta} -o {params.stem} {input.bim}"

rule rename_chrom:
    input:
        fasta = lambda wc: ref_fasta[wc["build"]],
    output:
        json = 'intermediate/cohort_proc/{cohort}.{build}.from{inputtype}_mapping.json',
        mapping = 'intermediate/cohort_proc/{cohort}.{build}.from{inputtype}_mapping.txt'
    threads: 1
    resources:
        mem_mb = 256,
        time_min = 10
    container: 'docker://befh/flippyr:0.5.3'
    script: 'scripts/rename_chrom.py'

# Recode sample plink file to vcf
rule plink_vcf:
    input:
        bed = "intermediate/cohort_proc/{cohort}.{build}.from{inputtype}_flipped.bed",
        bim = "intermediate/cohort_proc/{cohort}.{build}.from{inputtype}_flipped.bim",
        fam = "intermediate/cohort_proc/{cohort}.{build}.from{inputtype}_flipped.fam"
    output: temp("intermediate/cohort_proc/{cohort}.{build}.from{inputtype}.plink_contigs.vcf.gz")
    params:
        out = "intermediate/cohort_proc/{cohort}.{build}.from{inputtype}.plink_contigs"
    threads: 48
    resources:
        #mem_mb = 4000,
        mem_mb = 192000,
        time_min = 1440
    conda: "envs/plink2.yaml"
    shell:
        '''
plink2 --bed {input.bed} --bim {input.bim} --fam {input.fam} --recode vcf bgz \
  --real-ref-alleles --out {params.out} --threads {threads} --memory 160000 
'''


rule rename_vcf:
    input:
        vcf = rules.plink_vcf.output,
        mapping = rules.rename_chrom.output.mapping
    output: temp("intermediate/cohort_proc/{cohort}.{build}.from{inputtype}.vcf.gz")
    threads: 4
    resources:
        #mem_mb = 2048,
        mem_mb = 8192,
        time_min = 2880
    conda: "envs/bcftools.yaml"
    shell:
        '''
bcftools annotate --rename-chrs {input.mapping} -Oz -o {output} --threads {threads} {input.vcf}
'''

def choose_vcf_initial(wc):
    ds = wc['cohort']
    intype = input_type_dict[ds]
    if conf[ds].get('impute', False):
        if intype in ['bcf', 'vcf']:
            raise Exception("VCF or BCF imputation not yet supported")
        return 'output/imputation/data/{cohort}_chrall_filtered.vcf.gz'
    if intype == 'bcf':
        raise Exception("BCF not yet supported")
    elif intype == 'vcf':
        return conf[ds]['infile']
    else:
        return expand("intermediate/cohort_proc/{{cohort}}.{{build}}.from{inputtype}.vcf.gz",
                      inputtype = intype)

def choose_filter_vcf_initial(wc):
    ds = wc['cohort']
    intype = input_type_dict[ds]
    if intype == 'bcf':
        raise Exception("BCF not yet supported")
    elif intype == 'vcf':
        return filter_dict[ds]
    else:
        return ''

rule vcf:
    input: choose_vcf_initial
    output:
        vcf = temp("intermediate/cohort_proc/{cohort}.{build}.allchrom.vcf.gz"),
        tbi = "intermediate/cohort_proc/{cohort}.{build}.allchrom.vcf.gz.tbi"
    params:
        filt = choose_filter_vcf_initial
    conda: "envs/bcftools.yaml"
    threads: 1
    resources:
        mem_mb = 4000,
        time_min = 8600
    shell: '''
filt="{params.filt}"
if [[ -z "${{filt// }}" ]]; then
  cp {input} {output.vcf}
else
  bcftools view {params.filt} -Oz -o {output.vcf} {input}
fi
bcftools index -t {output.vcf}
'''

rule vcf_presplit:
    input: choose_vcf_initial
    output:
        vcf = temp("intermediate/{cohort}.{build}.chr{chrom}.perchrom.vcf.gz"),
        tbi = "intermediate/{cohort}.{build}.chr{chrom}.perchrom.vcf.gz.tbi"
    params:
        filt = lambda wc: filter_dict[wc['cohort']],
    conda: "envs/bcftools.yaml"
    threads: 1
    resources:
        mem_mb = 4000,
        time_min = 8600
    shell: '''
filt="{params.filt}"
if [[ -z "${{filt// }}" ]]; then
  ln -rs {input} {output.vcf}
else
  bcftools view {params.filt} -Oz -o {output.vcf} {input}
fi
bcftools index -t {output.vcf}
'''

rule vcf_merged:
    input:
        vcf = expand("intermediate/{{cohort}}.{{build}}.chr{chrom}.perchrom.vcf.gz", chrom = range(1,23)),
        tbi = expand("intermediate/{{cohort}}.{{build}}.chr{chrom}.perchrom.vcf.gz.tbi", chrom = range(1,23))
    output:
        vcf = temp("intermediate/{cohort}.{build}.mergedchrom.vcf.gz"),
        tbi = "intermediate/{cohort}.{build}.mergedchrom.vcf.gz.tbi"
    conda: "envs/bcftools.yaml"
    threads: 1
    resources:
        mem_mb = 4000,
        time_min = 8600
    shell: '''
bcftools concat -Oz -o {output.vcf} {input.vcf}
bcftools index -t {output.vcf}
'''

def choose_vcf_general(wc):
    ds = wc['cohort']
    intype = input_type_dict[ds]
    if intype == 'vcf' and re.match(r'.+\{chrom\}', conf[ds]['infile']):
        return rules.vcf_merged.output.vcf
    else:
        return rules.vcf.output.vcf

def choose_tbi_general(wc):
    return choose_vcf_general(wc) + '.tbi'

rule vcf_norm:
    input:
        vcf = choose_vcf_general,
        tbi = choose_tbi_general,
        fasta = lambda wc: ref_fasta[wc["build"]]
    output: "intermediate/{cohort}.{build}.vcf.gz"
    conda: "envs/bcftools.yaml"
    threads: 3
    resources:
        #mem_mb = 1024,
        mem_mb = 3072,
        time_min = 8600
    shell:
        """
bcftools norm -d none -f {input.fasta} {input.vcf} | \
bcftools annotate --threads 2 --set-id '%CHROM:%POS:%REF:%ALT' -Oz -o {output}
sleep 5
bcftools index -tf {output}
"""

rule process_smap_1kg_hgdp:
    input:
        metadata = HTTP.remote("storage.googleapis.com/gcp-public-data--gnomad/release/3.1.2/vcf/genomes/gnomad.genomes.v3.1.2.hgdp_1kg_subset_sample_meta.tsv.bgz")
    output:
        table = "resources/ref/proc/hgdp_1kg.popdata.tsv.gz",
        smap = "resources/ref/proc/hgdp_1kg.smap"
    threads: 1
    resources:
        mem_mb = 512,
        time_min = 5
    container: 'docker://befh/r_procadmix:v1'
    script: 'scripts/rule_process_smap_1kg_hgdp.R'


rule ref_admixture_filter:
    input: "/sc/arion/projects/load/data-ark/Public_Unrestricted/gnomAD/3.1/vcfs/HGDP-1KG/gnomad.genomes.v3.1.2.hgdp_tgp.chr{chrom}.vcf.bgz"
    output: temp('resources/ref/proc/{admixstem}.chr{chrom}.hg38.vcf.gz')
    conda: "envs/bcftools.yaml"
    threads: 4
    resources:
        #mem_mb = 2048,
        mem_mb = 8192,
        time_min = 360
    shell:
        '''
bcftools filter -i 'FILTER="PASS" && INFO/AF[0] >= 0.05 && INFO/AF[0] <= 0.95 && F_MISSING < 0.05' {input} | \
  bcftools annotate -x 'INFO,^FORMAT/GT' | \
  bcftools view --types snps -Oz -o {output}
sleep 5
bcftools index -tf {output}
'''

## VCF liftover module
liftover = {}
liftover['inputs'] = {
    ADMIXTURE_REF: {'input_vcf': f'resources/ref/proc/{ADMIXTURE_REF}.chr{{chrom}}.hg38.vcf.gz',
        'build': 'b38',
        'contigs': '1:22'}
}
liftover['output_builds'] = ['b37']
liftover['all_GATK_builds'] = True
liftover['concatenate'] = False
liftover['liftover_outdir'] = "resources/ref/lifted"

if 'vcf_liftover' not in config['pipeline_versions']:
    config['pipeline_versions']['vcf_liftover'] = '3aed94604072cca8ce6fd2245ee4710cb6aa8924'
elif config['pipeline_versions']['vcf_liftover'] == 'local':
    config['pipeline_versions']['vcf_liftover'] = os.path.join(local_src, 'vcf_liftover')
version_vcf_liftover = config['pipeline_versions']['vcf_liftover']

if re.search(r'/', version_vcf_liftover):
    sfile_vcf_liftover = os.path.join(
      os.path.abspath(version_vcf_liftover), 'workflow', 'Snakefile')
else:
    sfile_vcf_liftover = github('marcoralab/vcf_liftover',
        path='workflow/Snakefile', tag=version_vcf_liftover)

module vcf_liftover:
    snakefile: sfile_vcf_liftover
    config: liftover

use rule * from vcf_liftover as liftref_*

rule ref_link_lifted:
    input:
        vcf = 'resources/ref/lifted/{admixstem}.b37.chr{chrom}_only.vcf.gz',
        tbi = 'resources/ref/lifted/{admixstem}.b37.chr{chrom}_only.vcf.gz.tbi'
    output:
        vcf = 'resources/ref/proc/{admixstem}.chr{chrom}.hg19.vcf.gz',
        tbi = 'resources/ref/proc/{admixstem}.chr{chrom}.hg19.vcf.gz.tbi'
    localrule: True
    shell: '''
ln -rs {input.vcf} {output.vcf}
ln -rs {input.tbi} {output.tbi}
'''

rule ref_union_prune:
    input:
        vcf = 'resources/ref/proc/{admixstem}.chr{chrom}.{build}.vcf.gz',
        sample_vcf = rules.vcf_norm.output
    output: temp('intermediate/ref_pruned/by_chr/{admixstem}_pruned_{cohort}.chr{chrom}.{build}.vcf.gz')
    conda: "envs/bcftools.yaml"
    threads: 1
    resources:
        mem_mb = 2048,
        time_min = 360
    shell:
        '''
bcftools view -R {input.sample_vcf} --regions-overlap 2 {input.vcf} | \
  bcftools +prune -m r2=0.1 -w 100kb -Oz -o {output}
'''

rule ref_admixture_cat:
    input: expand('intermediate/ref_pruned/by_chr/{{admixstem}}_pruned_{{cohort}}.chr{chrom}.{{build}}.vcf.gz', chrom=range(1, 23))
    output: temp('intermediate/ref_pruned/{admixstem}_pruned_{cohort}.{build}.vcf.gz')
    conda: "envs/bcftools.yaml"
    threads: 1
    resources:
        mem_mb = 256,
        time_min = 5
    shell:
        '''
bcftools concat {input} | \
bcftools annotate --set-id '%CHROM:%POS:%REF:%ALT' -Oz -o {output}
'''

rule cohort_prune:
    input:
        ref_vcf = rules.ref_admixture_cat.output,
        vcf = rules.vcf_norm.output
    output: temp("intermediate/cohort_pruned/{cohort}_pruned_{admixstem}.{build}.vcf.gz")
    conda: "envs/bcftools.yaml"
    threads: 1
    resources:
        mem_mb = 2048,
        time_min = 360
    shell:
        '''
bcftools view -R {input.ref_vcf} --regions-overlap 2 {input.vcf} -Oz -o {output}
'''

rule ref_admixture_index:
    input: rules.ref_admixture_cat.output
    output: rules.ref_admixture_cat.output[0] + '.tbi'
    conda: "envs/bcftools.yaml"
    threads: 1
    resources:
        mem_mb = 1024,
        time_min = 360
    shell:
        '''
bcftools index -tf {input}
'''

rule cohort_index:
    input: rules.cohort_prune.output
    output: rules.cohort_prune.output[0] + '.tbi'
    conda: "envs/bcftools.yaml"
    threads: 1
    resources:
        mem_mb = 1024,
        time_min = 360
    shell:
        '''
bcftools index -tf {input}
'''

rule cohort_merge:
    input:
        cohort = rules.cohort_prune.output,
        ref = rules.ref_admixture_cat.output,
        reftbi = rules.ref_admixture_index.output,
        cohorttbi = rules.cohort_index.output
    output: temp("intermediate/cohort_pruned/{cohort}_merged_{admixstem}.{build}.vcf.gz")
    conda: "envs/bcftools.yaml"
    threads: 7
    resources:
        #mem_mb = 2048,
        mem_mb = 14336,
        time_min = 360
    shell:
        '''
bcftools merge {input.cohort} {input.ref} --threads 3 |
  bcftools norm -m+any |
  bcftools view --max-alleles 2 --threads 3 -Oz -o {output}
'''

rule ref_refilter:
    input:
        vcf = rules.ref_admixture_cat.output,
        tbi = rules.ref_admixture_index.output,
        merge_vcf = rules.cohort_merge.output
    output: temp("intermediate/ref_pruned/{admixstem}_pruned-filt_{cohort}.{build}.vcf.gz")
    conda: "envs/bcftools.yaml"
    threads: 4
    resources:
        #mem_mb = 2048,
        mem_mb = 8192,
        time_min = 360
    shell:
        '''
bcftools view -R {input.merge_vcf} --regions-overlap 2 \
  --threads {threads} -Oz -o {output} {input.vcf}
'''

rule ref_plink:
    input: rules.ref_refilter.output
    output: multiext('intermediate/ref_pruned/{admixstem}_pruned_{cohort}.{build}.', 'bed', 'bim', 'fam')
    params:
        out = lambda wc, output: output[0][:-4]
    conda: 'envs/plink.yaml'
    threads: 1
    resources:
        mem_mb = 256,
        time_min = 5
    shell:
        '''
plink --vcf {input} --double-id --keep-allele-order --make-bed --out {params.out}
'''

rule cohort_final_plink:
    input: rules.cohort_merge.output
    output: multiext("intermediate/cohort_pruned/{cohort}_merged_{admixstem}.{build}.", "bed", "bim", "fam")
    params:
        out = lambda wc, output: output[0][:-4]
    conda: 'envs/plink.yaml'
    threads: 1
    resources:
        mem_mb = 256,
        time_min = 5
    shell:
        '''
plink --vcf {input} --double-id --keep-allele-order --make-bed --out {params.out}
'''

rule check_bims:
    input:
        ref = 'intermediate/ref_pruned/{admixstem}_pruned_{cohort}.{build}.bim',
        cohort = 'intermediate/cohort_pruned/{cohort}_merged_{admixstem}.{build}.bim'
    output: 'intermediate/bim_check/{cohort}_{admixstem}.{build}.bim_diff.txt'
    threads: 1
    resources:
        mem_mb = 1024,
        time_min = 5
    shell: 'diff -s {input.ref} {input.cohort} &> {output}'

rule admixture:
    input:
        bed = "intermediate/ref_pruned/{admixstem}_pruned_{cohort}.{build}.bed",
        other = multiext("intermediate/ref_pruned/{admixstem}_pruned_{cohort}.{build}.", "bim", "fam"),
        check = rules.check_bims.output
    output:
        Qtemp = temp('{admixstem}_pruned_{cohort}.{build}.12.Q'),
        Ptemp = temp('{admixstem}_pruned_{cohort}.{build}.12.P'),
        Q = 'results/admixture_ref/{admixstem}_pruned_{cohort}.{build}.12.Q',
        P = 'results/admixture_ref/{admixstem}_pruned_{cohort}.{build}.12.P'
    log: 'results/admixture_ref/{admixstem}_pruned_{cohort}.{build}.12.log'
    threads: 40
    resources:
        #mem_mb = 5120, # Very efficient memory usage. Previous run used 2936 MB
        mem_mb = 204800,
        time_min = 1440 # 24 hours. Should take <= 14 hours
    container: 'docker://befh/r_env_gwasamplefilt:5'
    shell:
        '''
admixture -s 42 --cv {input.bed} 12 -j{threads} 2>&1 | tee {log}
cp {output.Qtemp} {output.Q}
cp {output.Ptemp} {output.P}
'''

rule admixture_project:
    input:
        bed = "intermediate/cohort_pruned/{cohort}_merged_{admixstem}.{build}.bed",
        other = multiext("intermediate/cohort_pruned/{cohort}_merged_{admixstem}.{build}.", "bim", "fam"),
        P = rules.admixture.output.P
    output:
        Qtemp = temp('{cohort}_merged_{admixstem}.{build}.12.Q'),
        Ptemp = temp('{cohort}_merged_{admixstem}.{build}.12.P'),
        P_in = temp('{cohort}_merged_{admixstem}.{build}.12.P.in'),
        Q = 'results/admixture_samp/{cohort}_merged_{admixstem}.{build}.12.Q',
        P = 'results/admixture_samp/{cohort}_merged_{admixstem}.{build}.12.P'
    log: 'results/admixture_samp/{cohort}_merged_{admixstem}.{build}.12.log'
    threads: 40
    resources:
        #mem_mb = 5120, # Very efficient memory usage. Previous run used 2936 MB
        mem_mb = 204800,
        time_min = 1440 # 24 hours. Should take <= 14 hours
    container: 'docker://befh/r_env_gwasamplefilt:5'
    shell:
        '''
cp {input.P} {output.P_in}
admixture -P -s 42 {input.bed} 12 -j{threads} 2>&1 | tee {log}
cp {output.Qtemp} {output.Q}
cp {output.Ptemp} {output.P}
'''

def fam_samp_orig(wc):
    ds = wc['cohort']
    intype = get_intype(conf, ds)
    if intype in ['plink', 'bplink']:
        return expand(rules.plink_filter.params.out + '.fam',
                      **wc, inputtype=intype)
    else:
        return []

rule process_admixture:
    input:
        fam_ref = 'intermediate/ref_pruned/{admixstem}_pruned_{cohort}.{build}.fam',
        q_samp = rules.admixture_project.output.Q,
        fam_samp = 'intermediate/cohort_pruned/{cohort}_merged_{admixstem}.{build}.fam',
        fam_samp_orig = fam_samp_orig,
        pops = rules.process_smap_1kg_hgdp.output.table
    output:
        rda = "results/admixture_cohort-{cohort}_ref-{admixstem}.{build}.rda",
        anc = "results/admixture_cohort-{cohort}_ref-{admixstem}.{build}.tsv"
    threads: 1
    resources:
        mem_mb = 2048,
        time_min = 5
    container: 'docker://befh/r_procadmix:v1'
    script: 'scripts/rule_process_admixture.R'

rule report_refadmix:
    input: rules.process_admixture.output.rda
    output: 'results/admixture_cohort-{cohort}_ref-{admixstem}.{build}.html'
    threads: 1
    resources:
        mem_mb = 4096,
        time_min = 10
    container: 'docker://befh/r_procadmix:v1'
    script: 'scripts/rule_report_refadmix.Rmd'
